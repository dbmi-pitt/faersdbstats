<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>stage_1_setup</name>
  <description/>
  <extended_description/>
  <job_version/>
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2022/02/14 11:07:21.196</created_date>
  <modified_user>-</modified_user>
  <modified_date>2022/02/14 11:07:21.196</modified_date>
  <parameters>
    </parameters>
  <connection>
    <name>${DATABASE_NAME}</name>
    <server>${DATABASE_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${DATABASE_NAME}</database>
    <port>${DATABASE_PORT}</port>
    <username>${DATABASE_USERNAME}</username>
    <password>${DATABASE_PASSWORD}</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${DATABASE_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection>${DATABASE_LOG_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <size_limit_lines>5000</size_limit_lines>
    <interval>1s</interval>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>Y</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>Y</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>Y</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>Y</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection>${DATABASE_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>Y</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection>${DATABASE_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file/>
  <entries>
    <entry>
      <name>Set AWS Credentials</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
export "AWS_ACCESS_KEY_ID=${AWS_S3_ACCESS_KEY}"
#echo "AWS_S3_ACCESS_KEY = ${AWS_S3_ACCESS_KEY}"

export "AWS_SECRET_ACCESS_KEY=${AWS_S3_SECRET_KEY}"
#echo "AWS_S3_BUCKET_NAME=${AWS_S3_BUCKET_NAME}"

export "AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}"

aws configure list</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>608</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>nda</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>T</sqlfromfile>
      <sqlfilename>${FAERSDBSTATS_REPO_LOCATION}/stage_1_setup/load_nda_table.sql</sqlfilename>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>400</xloc>
      <yloc>1264</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Drop all tables</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql>DROP SCHEMA ${DATABASE_SCHEMA} CASCADE;
CREATE SCHEMA ${DATABASE_SCHEMA};

GRANT ALL ON SCHEMA ${DATABASE_SCHEMA} TO ${DATABASE_USERNAME};
--GRANT ALL ON SCHEMA ${DATABASE_SCHEMA} TO public; --not sure if needed</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>912</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>SUCCESS IF LOAD_ALL_TIME=1</name>
      <description/>
      <type>SIMPLE_EVAL</type>
      <attributes/>
      <valuetype>variable</valuetype>
      <fieldname/>
      <variablename>${LOAD_ALL_TIME}</variablename>
      <fieldtype>string</fieldtype>
      <mask/>
      <comparevalue>1</comparevalue>
      <minvalue/>
      <maxvalue/>
      <successcondition>equal</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>336</xloc>
      <yloc>832</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Wait for 3s</name>
      <description/>
      <type>DELAY</type>
      <attributes/>
      <maximumTimeout>3</maximumTimeout>
      <scaletime>0</scaletime>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>688</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>country_codes_job</name>
      <description/>
      <type>JOB</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <job_object_id/>
      <filename>${Internal.Entry.Current.Directory}/country_codes_job.kjb</filename>
      <jobname/>
      <directory/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile>${Internal.Entry.Current.Directory}/../logs/country_codes_job</logfile>
      <logext>txt</logext>
      <add_date>Y</add_date>
      <add_time>Y</add_time>
      <loglevel>Rowlevel</loglevel>
      <slave_server_name/>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>Y</expand_remote_job>
      <create_parent_folder>Y</create_parent_folder>
      <pass_export>N</pass_export>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <set_append_logfile>Y</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>1264</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Create pdi_logging table</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>T</sqlfromfile>
      <sqlfilename>${FAERSDBSTATS_REPO_LOCATION}/stage_1_setup/create_pentaho_log_table.sql</sqlfilename>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>336</xloc>
      <yloc>688</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>orange_book_job</name>
      <description/>
      <type>JOB</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <job_object_id/>
      <filename>${FAERSDBSTATS_REPO_LOCATION}/stage_1_setup/orange_book_job.kjb</filename>
      <jobname/>
      <directory/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>Y</set_logfile>
      <logfile>${FAERSDBSTATS_REPO_LOCATION}/../logs/orange_book_job</logfile>
      <logext>txt</logext>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Minimal</loglevel>
      <slave_server_name/>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>N</expand_remote_job>
      <create_parent_folder>Y</create_parent_folder>
      <pass_export>N</pass_export>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <set_append_logfile>Y</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>1264</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Set job variables</name>
      <description/>
      <type>SET_VARIABLES</type>
      <attributes/>
      <replacevars>Y</replacevars>
      <filename>${Internal.Entry.Current.Directory}/../../faers_config.config</filename>
      <file_variable_type>JVM</file_variable_type>
      <fields>
        <field>
          <variable_name>BASE_FILE_DIR</variable_name>
          <variable_value>${BASE_FILE_DIR}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FAERSDBSTATS_REPO_LOCATION</variable_name>
          <variable_value>${FAERSDBSTATS_REPO_LOCATION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>CEM_DOWNLOAD_DATA_FOLDER</variable_name>
          <variable_value>${CEM_DOWNLOAD_DATA_FOLDER}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_BUCKET_NAME</variable_name>
          <variable_value>${AWS_S3_BUCKET_NAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_ACCESS_KEY</variable_name>
          <variable_value>${AWS_S3_ACCESS_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_SECRET_KEY</variable_name>
          <variable_value>${AWS_S3_SECRET_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_HOST</variable_name>
          <variable_value>${DATABASE_HOST}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_NAME</variable_name>
          <variable_value>${DATABASE_NAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_SCHEMA</variable_name>
          <variable_value>${DATABASE_SCHEMA}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_USERNAME</variable_name>
          <variable_value>${DATABASE_USERNAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_PASSWORD</variable_name>
          <variable_value>${DATABASE_PASSWORD}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_LOG_SCHEMA</variable_name>
          <variable_value>${DATABASE_LOG_SCHEMA}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_NEW_QUARTER</variable_name>
          <variable_value>${LOAD_NEW_QUARTER}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_NEW_YEAR</variable_name>
          <variable_value>${LOAD_NEW_YEAR}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_ALL_TIME</variable_name>
          <variable_value>${LOAD_ALL_TIME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>CEM_ORANGE_BOOK_DOWNLOAD_FILENAME</variable_name>
          <variable_value>${CEM_ORANGE_BOOK_DOWNLOAD_FILENAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOG_LOCATION</variable_name>
          <variable_value>${LOG_LOCATION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_BACKUP_LOCATION</variable_name>
          <variable_value>${DATABASE_BACKUP_LOCATION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>336</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Start</name>
      <description/>
      <type>SPECIAL</type>
      <attributes/>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>96</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>eu_drug_name_active_ingredient_job</name>
      <description/>
      <type>JOB</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <job_object_id/>
      <filename>${Internal.Entry.Current.Directory}/eu_drug_name_active_ingredient_job.kjb</filename>
      <jobname/>
      <directory/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>Y</set_logfile>
      <logfile>${FAERSDBSTATS_REPO_LOCATION}/../logs/eu_drug_job</logfile>
      <logext>txt</logext>
      <add_date>Y</add_date>
      <add_time>N</add_time>
      <loglevel>Minimal</loglevel>
      <slave_server_name/>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>N</expand_remote_job>
      <create_parent_folder>N</create_parent_folder>
      <pass_export>N</pass_export>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <set_append_logfile>Y</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>608</xloc>
      <yloc>1264</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>download_new_quarter_from_fda.sh</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>${Internal.Entry.Current.Directory}/download_new_quarter_from_fda.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>912</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>upload_new_quarter_from_fda.sh</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>${Internal.Entry.Current.Directory}/upload_new_quarter_from_fda.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>1136</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>create_schemas.sh</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>/home/pentaho-secondary/projects-brb265/faers/faersdbstats/stage_1_setup/create_schemas.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>96</xloc>
      <yloc>832</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>create_drug_usagi_mapping_table.sql</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql/>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>T</sqlfromfile>
      <sqlfilename>${FAERSDBSTATS_REPO_LOCATION}/stage_1_setup/create_drug_usagi_mapping_table.sql</sqlfilename>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>112</xloc>
      <yloc>1344</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Create Local TXT LOG</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/usr/bin/bash

# log_filename="LOG-"$(date +%B)"-"$(date +%Y)"-load.txt"
# log_filename="LOG-"${CURRENT_MONTH}"-"${CURRENT_YEAR}"-load.txt"
# export LOG_FILENAME=$log_filename

if [ ! -d ${BASE_FILE_DIR}/logs/ ]
then
  mkdir ${BASE_FILE_DIR}/logs/
fi
touch ${BASE_FILE_DIR}/logs/${LOG_FILENAME}
chmod 774 ${BASE_FILE_DIR}/logs/${LOG_FILENAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>608</xloc>
      <yloc>688</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>log_init.sh</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>/home/pentaho-secondary/projects-brb265/faers/faersdbstats/stage_1_setup/log_init.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>336</xloc>
      <yloc>768</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Log Drop All Tables</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
log_location=${BASE_FILE_DIR}/logs/${LOG_FILENAME}
echo "took the LOAD_ALL_TIME=1 path in stage_1_setup workflow " >> $log_location
echo "DROPPED ALL TABLES" >> $log_location</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>176</xloc>
      <yloc>912</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Log LOAD_ALL_TIME=0 path taken</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
log_location=${BASE_FILE_DIR}/logs/${LOG_FILENAME}
echo "took the LOAD_ALL_TIME=0 (red fail) path in stage_1_setup workflow " >> $log_location
echo "Will attempt to download LOAD_NEW_QUARTER's data" >> $log_location</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>608</xloc>
      <yloc>832</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Log STAGE 1 COMPLETE</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/usr/bin/bash
log_location=${BASE_FILE_DIR}/logs/${LOG_FILENAME}
printf '\n' >> $log_location
echo "#######################################################" >> $log_location
echo "################ STAGE 1 COMPLETE #####################" >> $log_location
echo "#######################################################" >> $log_location
printf '\n\n' >> $log_location</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>112</xloc>
      <yloc>1456</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Log FDA Download Success</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
log_location=${BASE_FILE_DIR}/logs/${LOG_FILENAME}
echo "FDA Download success... ${BASE_FILE_DIR}/data_new " >> $log_location
echo "Will attempt to upload LOAD_NEW_QUARTER's data" >> $log_location</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>912</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Log FDA Data Upload to S3 Success</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
log_location=${BASE_FILE_DIR}/logs/${LOG_FILENAME}
echo "FDA Data Upload to S3 Success... https://s3.console.aws.amazon.com/s3/buckets/${AWS_S3_BUCKET_NAME}?prefix=data/faers/demo/${LOAD_NEW_YEAR}/&amp;region=${AWS_DEFAULT_REGION} " >> $log_location
echo "\n please check that new data is in the s3 bucket here ^ and other domains like drug, indi, etc..." >> $log_location</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>1136</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>db backup</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>rundate=$(date "+%m%d%Y")
#looks nice if your ${LOG_FILENAME_ROOT} is something like "LOG-Sept-2022-load"
#pg_dump -d ${DATABASE_NAME} -h ${DATABASE_HOST} -p ${DATABASE_PORT} -U ${DATABASE_USERNAME} -W ${DATABASE_PASSWORD} > ${DATABASE_BACKUP_LOCATION}/${LOG_FILENAME_ROOT: 4}_${rundate}.dump
# '/usr/bin/postgresql/10/bin/pg_dump' –U rw_grp cem_pitt_2022 > /media/large-backup-drive/pentaho-user-dumps/Sept-2022-load_09122022.dump
# pg_dump cem_pitt_2022 > /media/large-backup-drive/pentaho-user-dumps/Sept-2022-load_09132022.dump
psql -h ${DATABASE_HOST} -p ${DATABASE_PORT} "user=${DATABASE_USER} dbname=${DATABASE_NAME} password=\'${DATABASE_PASSWORD}\'" 
pg_dump cem_pitt_2022 > /media/large-backup-drive/pentaho-user-dumps/Sept-2022-load_09192022.dump</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>336</xloc>
      <yloc>608</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>download fda laers data</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>${Internal.Entry.Current.Directory}/data_from_fda_scripts/download_laers_fda_data.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>1008</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>download fda faers data</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename>${Internal.Entry.Current.Directory}/data_from_fda_scripts/download_faers_fda_data.sh</filename>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>N</insertScript>
      <script/>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>1072</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>if REBUILD_ALL_TIME_DATA_LOCALLY = 1</name>
      <description/>
      <type>SIMPLE_EVAL</type>
      <attributes/>
      <valuetype>variable</valuetype>
      <fieldname/>
      <variablename>${REBUILD_ALL_TIME_DATA_LOCALLY}</variablename>
      <fieldtype>string</fieldtype>
      <mask/>
      <comparevalue>1</comparevalue>
      <minvalue/>
      <maxvalue/>
      <successcondition>equal</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>1008</yloc>
      <attributes_kjc/>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>Start</from>
      <to>Set job variables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Set job variables</from>
      <to>Set AWS Credentials</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Wait for 3s</from>
      <to>Create pdi_logging table</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>orange_book_job</from>
      <to>nda</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>country_codes_job</from>
      <to>orange_book_job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>SUCCESS IF LOAD_ALL_TIME=1</from>
      <to>Drop all tables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>nda</from>
      <to>eu_drug_name_active_ingredient_job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>create_schemas.sh</from>
      <to>SUCCESS IF LOAD_ALL_TIME=1</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>eu_drug_name_active_ingredient_job</from>
      <to>create_drug_usagi_mapping_table.sql</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Create pdi_logging table</from>
      <to>Create Local TXT LOG</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Create Local TXT LOG</from>
      <to>log_init.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>log_init.sh</from>
      <to>create_schemas.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Drop all tables</from>
      <to>Log Drop All Tables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>SUCCESS IF LOAD_ALL_TIME=1</from>
      <to>Log LOAD_ALL_TIME=0 path taken</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Log LOAD_ALL_TIME=0 path taken</from>
      <to>download_new_quarter_from_fda.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>create_drug_usagi_mapping_table.sql</from>
      <to>Log STAGE 1 COMPLETE</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Log FDA Download Success</from>
      <to>upload_new_quarter_from_fda.sh</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>download_new_quarter_from_fda.sh</from>
      <to>Log FDA Download Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>upload_new_quarter_from_fda.sh</from>
      <to>Log FDA Data Upload to S3 Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Log FDA Data Upload to S3 Success</from>
      <to>country_codes_job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Set AWS Credentials</from>
      <to>db backup</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>db backup</from>
      <to>Wait for 3s</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>N</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>download fda faers data</from>
      <to>country_codes_job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Log Drop All Tables</from>
      <to>if REBUILD_ALL_TIME_DATA_LOCALLY = 1</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>if REBUILD_ALL_TIME_DATA_LOCALLY = 1</from>
      <to>download fda laers data</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>if REBUILD_ALL_TIME_DATA_LOCALLY = 1</from>
      <to>country_codes_job</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>download fda laers data</from>
      <to>download fda faers data</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
    <notepad>
      <note>Stage 1 - Set-up Reference and Mapping Data

Preq's
- having run stage_0_set_pentaho_vars.kjb

ABOUT EACH STEP
-set job variables						-brings in environment variables
-Create pdi_logging table			-creates the pentaho logging database tables
-update LOG SQL						-updates pdi_logging for pentaho 9
SUCCESS IF LOAD_ALL_TIME=1		**USE CASE LOGIC***
-orange_book_job					-downloads s3 data and uploads it to s3 then inputs it into the database w/ orange_book_transform
-nda													-create table and index if not exists
-eu_active_job						-was eu_registered_drugs_by_active_ingredent_job, creates table, uploads to s3, and inputs data w/ eu transform



To-do:

Troubleshooting:

Current Issues:



</note>
      <xloc>0</xloc>
      <yloc>0</yloc>
      <width>1009</width>
      <heigth>435</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>&lt;&lt;LOAD_ALL_TIME value from faers_config.config

if one will drop all and load all "fresh"</note>
      <xloc>768</xloc>
      <yloc>832</yloc>
      <width>343</width>
      <heigth>61</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>&lt;&lt; even though drug_usagi isn't completed in this workflow
this table structure allows for stage 4 sqls to run w/o error</note>
      <xloc>288</xloc>
      <yloc>1360</yloc>
      <width>404</width>
      <heigth>44</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
    <notepad>
      <note>needs upload to s3 step
see stage_1_setup/data_from_fda/readme.md</note>
      <xloc>0</xloc>
      <yloc>1136</yloc>
      <width>324</width>
      <heigth>44</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
    </group>
  </attributes>
</job>
