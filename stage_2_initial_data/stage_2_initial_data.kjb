<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>stage_2_initial_data</name>
  <description/>
  <extended_description/>
  <job_version/>
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2022/02/14 11:07:21.196</created_date>
  <modified_user>-</modified_user>
  <modified_date>2022/02/14 11:07:21.196</modified_date>
  <parameters>
    </parameters>
  <connection>
    <name>${DATABASE_NAME}</name>
    <server>${DATABASE_HOST}</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>${DATABASE_NAME}</database>
    <port>5432</port>
    <username>${DATABASE_USERNAME}</username>
    <password>${DATABASE_PASSWORD}</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>5432</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection>${DATABASE_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <size_limit_lines>5000</size_limit_lines>
    <interval>1s</interval>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>Y</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>Y</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>Y</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>Y</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection>${DATABASE_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>Y</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection>${DATABASE_NAME}</connection>
    <schema>${DATABASE_LOG_SCHEMA}</schema>
    <table>${DATABASE_LOG_NAME}</table>
    <timeout_days>3.0</timeout_days>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file/>
  <entries>
    <entry>
      <name>set AWS credentials</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>#!/bin/bash
export "AWS_ACCESS_KEY_ID=${AWS_S3_ACCESS_KEY}"
#echo "AWS_S3_ACCESS_KEY = ${AWS_S3_ACCESS_KEY}"

export "AWS_SECRET_ACCESS_KEY=${AWS_S3_SECRET_KEY}"
#echo "AWS_S3_BUCKET_NAME=${AWS_S3_BUCKET_NAME}"

export "AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}"

aws configure list</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>560</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Create Legacy Database Structures</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql>--brian buck changes
--commented out drop's and truncates
--added "if not exists" to creates
--index and binary tree commented out

set search_path = ${DATABASE_SCHEMA};
--demo

--drop table if exists demo_legacy;
CREATE TABLE IF NOT EXISTS demo_legacy (
	isr varchar NULL,
	"CASE" varchar NULL,
	i_f_cod varchar NULL,
	foll_seq varchar NULL,
	image varchar NULL,
	event_dt varchar NULL,
	mfr_dt varchar NULL,
	fda_dt varchar NULL,
	rept_cod varchar NULL,
	mfr_num varchar NULL,
	mfr_sndr varchar NULL,
	age varchar NULL,
	age_cod varchar NULL,
	gndr_cod varchar NULL,
	e_sub varchar NULL,
	wt varchar NULL,
	wt_cod varchar NULL,
	rept_dt varchar NULL,
	occp_cod varchar NULL,
	death_dt varchar NULL,
	to_mfr varchar NULL,
	confid varchar NULL,
	reporter_country varchar NULL,
	filename varchar NULL
);
--truncate demo_legacy;


--drug
--drop table if exists drug_legacy;
create table IF NOT EXISTS drug_legacy
(
ISR varchar,
DRUG_SEQ varchar,
ROLE_COD varchar,
DRUGNAME varchar,
VAL_VBM varchar,
ROUTE varchar,
DOSE_VBM varchar,
DECHAL varchar,
RECHAL varchar,
LOT_NUM varchar,
EXP_DT varchar,
NDA_NUM varchar,
FILENAME varchar
);
--truncate drug_legacy;

--indi

--drop table if exists indi_legacy;
CREATE TABLE IF NOT EXISTS indi_legacy (
	isr varchar NULL,
	drug_seq varchar NULL,
	indi_pt varchar NULL,
	filename varchar NULL
);
--truncate indi_legacy;


--outc
--drop table if exists outc_legacy;
CREATE TABLE IF NOT EXISTS outc_legacy (
	isr varchar NULL,
	outc_cod varchar NULL,
	filename varchar NULL
);
--truncate outc_legacy;


--reac
--drop table if exists reac_legacy;
CREATE TABLE IF NOT EXISTS reac_legacy (
	isr varchar NULL,
	pt varchar NULL,
	filename varchar NULL
);

--truncate reac_legacy;

--CREATE INDEX ix_reac_legacy_1 ON reac_legacy USING btree (upper((pt)::text));
--CREATE INDEX ix_reac_legacy_2 ON reac_legacy USING btree (isr);


--reac_pt_legacy
--drop table if exists reac_pt_legacy_list ;
CREATE TABLE IF NOT EXISTS reac_pt_legacy_list (
	isr varchar NULL,
	reac_pt_list text NULL
);
--truncate reac_pt_legacy_list;


--rpsr_legacy
--drop table if exists rpsr_legacy;
CREATE TABLE IF NOT EXISTS rpsr_legacy (
	isr varchar NULL,
	rpsr_cod varchar NULL,
	filename varchar NULL
);
--truncate rpsr_legacy;


--ther_legacy
--drop table if exists ther_legacy ;
CREATE TABLE IF NOT EXISTS ther_legacy (
	isr varchar NULL,
	drug_seq varchar NULL,
	start_dt varchar NULL,
	end_dt varchar NULL,
	dur varchar NULL,
	dur_cod varchar NULL,
	filename varchar NULL
);
--truncate ther_legacy ;


--all_casedemo
--drop table if exists all_casedemo;
CREATE TABLE IF NOT EXISTS all_casedemo (
	"database" text NULL,
	caseid varchar NULL,
	isr varchar NULL,
	caseversion varchar NULL,
	i_f_code varchar NULL,
	event_dt varchar NULL,
	age varchar NULL,
	sex varchar NULL,
	reporter_country varchar NULL,
	primaryid varchar NULL,
	drugname_list text NULL,
	reac_pt_list text NULL,
	fda_dt varchar NULL,
	imputed_field_name text NULL
);
--truncate all_casedemo;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>1024</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>create domain tables</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql>--if LOAD_ALL_TIME drops &amp; creates domain tables
--truncates are not needed if LOAD_ALL_TIME because dropped then created; and not needed if just adding new quarter of data

set search_path = ${DATABASE_SCHEMA};

--demo

--drop table if exists demo;
CREATE TABLE IF NOT EXISTS demo (
	primaryid varchar NULL,
	caseid varchar NULL,
	caseversion varchar NULL,
	i_f_code varchar NULL,
	event_dt varchar NULL,
	mfr_dt varchar NULL,
	init_fda_dt varchar NULL,
	fda_dt varchar NULL,
	rept_cod varchar NULL,
	auth_num varchar NULL,
	mfr_num varchar NULL,
	mfr_sndr varchar NULL,
	lit_ref varchar NULL,
	age varchar NULL,
	age_cod varchar NULL,
	age_grp varchar NULL,
	sex varchar NULL,
	e_sub varchar NULL,
	wt varchar NULL,
	wt_cod varchar NULL,
	rept_dt varchar NULL,
	to_mfr varchar NULL,
	occp_cod varchar NULL,
	reporter_country varchar NULL,
	occr_country varchar NULL,
	filename varchar NULL
);
--truncate demo;


--drug

--drop table if exists drug;
CREATE TABLE IF NOT EXISTS drug (
	primaryid varchar NULL,
	caseid varchar NULL,
	drug_seq varchar NULL,
	role_cod varchar NULL,
	drugname varchar NULL,
	prod_ai varchar NULL,
	val_vbm varchar NULL,
	route varchar NULL,
	dose_vbm varchar NULL,
	cum_dose_chr varchar NULL,
	cum_dose_unit varchar NULL,
	dechal varchar NULL,
	rechal varchar NULL,
	lot_num varchar NULL,
	exp_dt varchar NULL,
	nda_num varchar NULL,
	dose_amt varchar NULL,
	dose_unit varchar NULL,
	dose_form varchar NULL,
	dose_freq varchar NULL,
	filename varchar NULL
);
--truncate drug;


--indi

--drop table if exists indi ;
create table IF NOT EXISTS indi
(
primaryid varchar,
caseid varchar,
indi_drug_seq varchar,
indi_pt varchar,
filename varchar
);
--truncate indi;
--CREATE INDEX IF NOT EXISTS indi_caseid_idx ON frs.indi USING btree (caseid); --NOTE FRS does not exist
CREATE INDEX IF NOT EXISTS indi_filename_idx ON indi USING btree (filename);



--outc
--drop table if exists outc ;
CREATE TABLE IF NOT EXISTS outc (
	primaryid varchar NULL,
	caseid varchar NULL,
	outc_code varchar NULL,
	filename varchar NULL
);
--truncate outc;
CREATE INDEX IF NOT EXISTS outc_caseid_idx ON outc USING btree (caseid);
CREATE INDEX IF NOT EXISTS outc_filename_idx ON outc USING btree (filename);


--reac

--drop table if exists reac ;
CREATE TABLE IF NOT EXISTS reac (
	primaryid varchar NULL,
	caseid varchar NULL,
	pt varchar NULL,
	drug_rec_act varchar NULL,
	filename varchar NULL
);
--truncate reac ;
CREATE INDEX IF NOT EXISTS ix_reac_1 ON reac USING btree (upper((pt)::text));
CREATE INDEX IF NOT EXISTS ix_reac_2 ON reac USING btree (primaryid);
CREATE INDEX IF NOT EXISTS reac_caseid_idx ON reac USING btree (caseid);
CREATE INDEX IF NOT EXISTS reac_filename_idx ON reac USING btree (filename);


--rpsr

--drop table if exists rpsr;
CREATE TABLE IF NOT EXISTS rpsr (
	primaryid varchar NULL,
	caseid varchar NULL,
	rpsr_cod varchar NULL,
	filename varchar NULL
);
--truncate rpsr;
CREATE INDEX IF NOT EXISTS rpsr_caseid_idx ON rpsr USING btree (caseid, primaryid);
CREATE INDEX IF NOT EXISTS rpsr_filename_idx ON rpsr USING btree (filename);

--ther

--drop table if exists ther ;
CREATE TABLE IF NOT EXISTS ther (
	primaryid varchar NULL,
	caseid varchar NULL,
	dsg_drug_seq varchar NULL,
	start_dt varchar NULL,
	end_dt varchar NULL,
	dur varchar NULL,
	dur_cod varchar NULL,
	filename varchar NULL
);
--truncate ther;
</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>752</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>s3_data_download</name>
      <description/>
      <type>JOB</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <job_object_id/>
      <filename>${Internal.Entry.Current.Directory}/s3_data_download.kjb</filename>
      <jobname/>
      <directory/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Nothing</loglevel>
      <slave_server_name/>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>Y</expand_remote_job>
      <create_parent_folder>N</create_parent_folder>
      <pass_export>N</pass_export>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <set_append_logfile>N</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>1040</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>create_meddra_snomed_mapping.sql</name>
      <description/>
      <type>SQL</type>
      <attributes/>
      <sql>-- Create a mapping table from MedDRA to SNOMED-CT. 
-- We will use this table to map indications and reactions from MedDRA preferred term concepts to SNOMED-CT concepts/
--
-- LTS Computing LLC
-----------------------------------------------------------------------------------------------
set search_path = ${DATABASE_SCHEMA};

--drop table if exists meddra_snomed_mapping;
create table if not exists meddra_snomed_mapping as
SELECT z.SNOMED_CONCEPT_ID, z.SNOMED_CONCEPT_NAME, z.SNOMED_CONCEPT_CODE, z.MEDDRA_CONCEPT_ID, z.MEDDRA_CONCEPT_NAME, z.MEDDRA_CONCEPT_CODE, z.MEDDRA_CLASS_ID
FROM (
      SELECT ca.max_levels_of_separation, ca.min_levels_of_separation, c.concept_id AS MEDDRA_CONCEPT_ID,
	     c.concept_code AS MEDDRA_CONCEPT_CODE, c.concept_name AS MEDDRA_CONCEPT_NAME, c.concept_class_id AS MEDDRA_CLASS_ID,
	     c2.concept_id AS SNOMED_CONCEPT_ID, c2.concept_name AS SNOMED_CONCEPT_NAME, c2.concept_code AS SNOMED_CONCEPT_CODE,
	     ROW_NUMBER() OVER(PARTITION BY c.CONCEPT_ID ORDER BY c.CONCEPT_ID, ca.min_levels_of_separation, ca.max_levels_of_separation, c.CONCEPT_ID, c2.CONCEPT_ID) AS ROW_NUM
      FROM staging_vocabulary.CONCEPT c JOIN staging_vocabulary.concept_ancestor ca ON ca.ancestor_concept_id = c.concept_id
      JOIN staging_vocabulary.CONCEPT c2 ON c2.concept_id = ca.descendant_concept_id
	     AND c2.vocabulary_id = 'SNOMED'
	     AND c2.CONCEPT_CLASS_ID = 'Clinical Finding'
	     AND c2.INVALID_REASON IS NULL
      WHERE c.vocabulary_id = 'MedDRA'
      AND c.INVALID_REASON IS NULL
) z
WHERE z.ROW_NUM = 1;
</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>${DATABASE_NAME}</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>768</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Wait for 3s</name>
      <description/>
      <type>DELAY</type>
      <attributes/>
      <maximumTimeout>3</maximumTimeout>
      <scaletime>0</scaletime>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>320</xloc>
      <yloc>672</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>set job variables</name>
      <description/>
      <type>SET_VARIABLES</type>
      <attributes/>
      <replacevars>Y</replacevars>
      <filename>${Internal.Entry.Current.Directory}/../../faers.config</filename>
      <file_variable_type>JVM</file_variable_type>
      <fields>
        <field>
          <variable_name>FILE_DIR</variable_name>
          <variable_value>${BASE_FILE_DIR}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FAERSDBSTATS_REPO_LOCATION</variable_name>
          <variable_value>${FAERSDBSTATS_REPO_LOCATION}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>CEM_DOWNLOAD_DATA_FOLDER</variable_name>
          <variable_value>${CEM_DOWNLOAD_DATA_FOLDER}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_BUCKET_NAME</variable_name>
          <variable_value>${AWS_S3_BUCKET_NAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_ACCESS_KEY</variable_name>
          <variable_value>${AWS_S3_ACCESS_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>AWS_S3_SECRET_KEY</variable_name>
          <variable_value>${AWS_S3_SECRET_KEY}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_HOST</variable_name>
          <variable_value>${DATABASE_HOST}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_NAME</variable_name>
          <variable_value>${DATABASE_NAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_SCHEMA</variable_name>
          <variable_value>${DATABASE_SCHEMA}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_USERNAME</variable_name>
          <variable_value>${DATABASE_USERNAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_PASSWORD</variable_name>
          <variable_value>${DATABASE_PASSWORD}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DATABASE_LOG_SCHEMA</variable_name>
          <variable_value>${DATABASE_LOG_SCHEMA}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_NEW_QUARTER</variable_name>
          <variable_value>${LOAD_NEW_QUARTER}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_NEW_YEAR</variable_name>
          <variable_value>${LOAD_NEW_YEAR}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>LOAD_ALL_TIME</variable_name>
          <variable_value>${LOAD_ALL_TIME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>CEM_ORANGE_BOOK_DOWNLOAD_FILENAME</variable_name>
          <variable_value>${CEM_ORANGE_BOOK_DOWNLOAD_FILENAME}</variable_value>
          <variable_type>JVM</variable_type>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>320</xloc>
      <yloc>560</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Start</name>
      <description/>
      <type>SPECIAL</type>
      <attributes/>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>128</xloc>
      <yloc>560</yloc>
      <attributes_kjc/>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>Start</from>
      <to>set job variables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>set job variables</from>
      <to>set AWS credentials</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>create_meddra_snomed_mapping.sql</from>
      <to>create domain tables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>s3_data_download</from>
      <to>Create Legacy Database Structures</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>create domain tables</from>
      <to>s3_data_download</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>set AWS credentials</from>
      <to>Wait for 3s</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Wait for 3s</from>
      <to>create_meddra_snomed_mapping.sql</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
    <notepad>
      <note>Preq's
- having run stage_1_setup.kjb

ABOUT EACH STEP
-omop											-most excellently creates omop data
-create meddra snomed mapping
-set job variables 2					-exact same as before (brings in env vars)
-drop create domain tables		-only if LOAD_ALL_TIME (=1) then drop and create fresh tables else do nothing
-s3 data download						-mkdir ${BASE_FILE_DIR}/data_from_s3/ then if LOAD_ALL_TIME then download all data from to ${BASE_FILE_DIR}/data_from_s3/ else specific year quarter info
-domain transform						-put the downloaded domains' data from data_from_s3 folder into the database
-Create Legacy Database Structures (no drop; create; no truncate)				-truncates commented out
-derive_unique_all_case.sql			-makes LAERS data like FAERS data and removes duplicates created in process


To-do: reestablish database connection so that meta works without having to run orange_book_job first.
update s3 bucket location to faers and laers

Troubleshooting Tips:
if "Logging" tab stops producing output restart spoon
if Java Null Pointer Error (and/or job 'job") restart spoon then rebuild failing step as if new

Current Issues:
Logging is blank

</note>
      <xloc>16</xloc>
      <yloc>0</yloc>
      <width>1343</width>
      <heigth>435</heigth>
      <fontname>Ubuntu</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
    </group>
  </attributes>
</job>
